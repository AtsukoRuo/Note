# 数据库

[TOC]

## 范式

![image-20240611021111121](./assets/image-20240611021111121.png)

最好保证数据库的设计达到第三范式的要求

## 分库分表

分库分表的必然性：

1. 业务拆分的必然性
2. 写入性能达到瓶颈
3. 数据隔离



分库分表后，我们会遇到以下问题：

- **查询维度问题**。假设我们有一张表，它有两个查询维度 A、B。现在我们按 A 维度进行拆分。按 A 进行查询，很容易定位到拆分后的某张表上，但是按照 B 进行查询，则异常困难。对于在分库分表之后其他维度的查询，一般有以下几个方法：
  1. 建立从 B 到 A 的映射表，在按 B 查询时，从映射表中获取对应的 A ID，然后再根据 A ID 从表中获取数据
  2. 业务双写：同一份数据，按 A、B 分别进行拆表，维护两套表。
  3. 异步双写：也是维护两套表，但通过监听 Binlog，将变更同步到另外一套表上。从而实现业务单写。
  4. 两个维度统一到一个维度，即 A ID 与 B ID 某几位是相同的，用这几位作为分库维度。
-  Join 查询问题：
  1. 把 Join 拆成多个单表查询，在代码层对结果进行拼装
  2. 先提前做 Join 操作，将结果存放在一张表中，这就是「空间换时间」的思路。
  3. 将数据放在 ES 中，在搜索引擎中进行查询。
- 事务问题：单独说明



## B+ Tree

关系型数据库在查询方面有一些重要特性：

1. 范围查询
2. 前缀匹配模糊查询
3. 排序和分页

这得益于  B+ 树这种数据结构，它的逻辑结构如下：

![image-20240611105315483](./assets/image-20240611105315483.png)

- 在叶子节点一层，所有记录的主键按照从小到大的顺序排列，并且形成了一个双向链表。
- 非叶子节点取的是叶子节点里面 Key 的最小值。

以上图为例，我们说明如何实现上述特性：

- 范围查询。比如要查主键在`[1,17]`之间的记录。二次查询，先查找 1 所在的叶子节点的记录位置，再查找 17 所在的叶子节点记录的位置（就是16所处的位置），然后顺序地从 1 遍历链表直到 16 所在的位置。
- 前缀匹配模糊查询。假设主键是一个字符串类型，要查询 `where Key like abc%` 其实可以转化成一个范围查询 `Key in [abc,abcz]`。当然，如果是后缀匹配模糊查询，或者诸如 where Key like %c% 这样的中间匹配，则没有办法转化成范围查询，只能挨个遍历。
- 叶子节点天然是排序好的，支持排序和分页。

对于 `select xxxwhere xxx limit 1000,10`，需要遍历前面 1000 个叶子节点，才能获取数据。我们将 `offset=1000` 的位置换算成某个`max_id`，然后可以使用 `select xxx where xxx and id>max_id limit 10`，这样就可以利用 B+  树的特性，快速定位到 `max_id` 所在的位置，即是 `offset=1000` 所在的位置。

在 InnoDB 的实践里面，其中一个建议是按主键的自增顺序插入记录，就是为了避免节点分裂现象。

## 索引

InnoDB 索引要点：

- 每个索引都有一颗 B+ 树
- 非主键索引的叶子节点存储的不是数据记录的指针，而是主键的值。所以，对于非主键索引的查询，会查询两棵 B+ 树
- 主键必有一个索引
- 主键`(a,b)`的索引结构是先按 a 排序，再按 b 排序。

聚簇索引的叶子节点就是数据节点，也就是说索引和数据行在一起；反之，如果叶子节点没有存储数据行，那么就是非聚簇索引。聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。`innodb`中的主键索引采用的是聚簇索引。

索引的类型有：

1. 普通索引（由关键字 KEY 或 INDEX 定义），值可以重复
2. 唯一索引（由关键字 UNIQUE 定义）要求值必须唯一，但允许有空值
3. 主键索引（由关键字 PRIMARY 定义）

只包含一个字段的索引叫做**单列索引**，包含两个或以上字段的索引叫做**复合索引**（或组合索引）。下面这个 SQL 语句在 列 X，列 Y，列 Z 上建立了一个复合索引。

```mysql
CREATE INDEX 索引名 ON 表名(列名X, 列名Y, 列名Z);
```

其实这相当于建立了三个索引，分别是：

1. 单列索引（列X）
2. 复合索引（列X, 列Y）
3. 复合索引（列X，列Y，列Z）

~~~sql
SELECT * FROM XXX WHERE X = 1 AND Y = 2	-- 使用了索引
SELECT * FROM XXX WHERE  Y = 2		    -- 未使用索引
~~~

这就是**组合索引「最左前缀」**的结果。



## 事务

事务是指一组读写操作的逻辑单元。**事务往往会被赋予AID属性**，来保证数据的一致性（**C**onsistency）：

- **原子性**（**A**tomic）：，事务保证了在同一项业务处理过程中，对多个数据的修改，要么同时成功，要么同时被撤销。
- **隔离性**（**I**solation）：事务保证了在不同的业务处理过程中，各自业务正在读、写的数据互相独立，不会彼此影响。
- **持久性**（**D**urability）：事务应当保证所有成功被提交的数据修改都能够正确地被持久化，不丢失数据。
- **一致性**（**C**onsistency）：数据库完整性约束

**可见，A、I、D 是手段，C 是目的**。

- **内部一致性**：一个服务只使用一个数据源时的事务间一致性。

  通过 A、I、D 来获得一致性是最经典的做法。此时，多个并发事务所读写的数据能够**由数据源感知是否存在冲突**，并发事务的读写在时间线上的最终顺序是由数据源来确定的。

- **外部一致性**：一个服务使用到多个不同的数据源时的事务间一致性

  外部一致性问题通常很难再使用 A、I、D 来解决，但是外部一致性是分布式系统中必然会遇到的问题。为此，我们要转变观念，即从「是或否」的二元属性转变为可以按不同强度分开讨论的多元属性。在确保代价可承受的前提下，获得强度尽可能高的一致性保障。

**数据源是指提供数据的逻辑设备**

>如今要研究事务的实现原理，必定会追溯到**ARIES 理论（Algorithms for Recovery and Isola tion Exploiting Semantics，ARIES，基于语义的恢复与隔离算法）**。
>
>在 20 世纪 90 年代，[IBM Almaden 研究院](http://www.research.ibm.com/labs/almaden/)总结了研发原型数据库系统“IBM System R”的经验，发表了 ARIES 理论中最主要的三篇论文，其中《[ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging](https://cs.stanford.edu/people/chrismre/cs345/rl/aries.pdf)》着重解决了 ACID 的其中两个属性：原子性（A）和持久性（D）在算法层面上应当如何实现。而另一篇《[ARIES/KVL: A Key-Value Locking Method for Concurrency Control of Multiaction Transactions Operating on B-Tree Indexes](http://vldb.org/conf/1990/P392.PDF)》则是现代数据库隔离性（I）奠基式的文章

## 隔离性

数据库通过锁机制来实现隔离性：

- **写锁**（Write Lock，也叫作排他锁，eXclusive Lock，简写为 X-Lock）：如果数据被加上写锁，就只有持有写锁的事务才能对数据进行写入操作，而其他事务不能写入数据，也不能施加读锁或者写锁。写锁禁止其他事务施加读锁，而并不禁止其他事务读取数据。

- **读锁**（Read Lock，也叫作共享锁，Shared Lock，简写为 S-Lock）：多个事务可以对同一个数据添加多个读锁，数据被加上读锁后就不能再被加上写锁，而且其他事务不能对该数据进行写入，但仍然可以读取。

  对于持有读锁的事务，如果该数据只有它自己一个事务加了读锁，允许直接将其升级为写锁，然后写入数据。但是不允许反向降级。

- **范围锁**（Range Lock）：加了范围锁后，不仅无法修改该范围内已有的数据，也不能在该范围内新增或删除任何数据，这是排他锁无法做到的

  

  

下面是不同级别的隔离性，依次从强到弱：

- **可串行化**：对事务所有读、写的数据全都加上写锁和范围锁，或者读锁和范围锁，一直持续到事务结束。串行化访问提供了强度最高的隔离性。但是[并发控制理论](https://en.wikipedia.org/wiki/Concurrency_control)（Concurrency Control）阐述了隔离程度与并发能力是负相关的，即隔离程度越高，并发访问时的吞吐量就越低。

- **可重复读（Repeatable Read）**：对事务所涉及的数据加读锁或者写锁，一直持续到事务结束。

  它存在[幻读问题](https://en.wikipedia.org/wiki/Isolation_(database_systems)#Phantom_reads)（Phantom Reads），例如：

  ~~~sql
  SELECT count(1) FROM books WHERE price < 100					/* 时间顺序：1，事务： T1  暂不提交*/
  INSERT INTO books(name,price) VALUES ('深入理解Java虚拟机',90)	/* 时间顺序：2，事务： T2  提交*/
  SELECT count(1) FROM books WHERE price < 100					/* 时间顺序：3，事务： T1 提交*/
  ~~~

  两次相同的查询就会得到不一样的结果，原因是可重复读没有范围锁，来禁止在该范围内插入新的数据。

- **读已提交（Read Committed）**：对事务涉及的数据加的写锁，一直持续到事务结束，但加在查询操作上的读锁，在查询完成后就马上会释放。

  它存在[不可重复读问题](https://en.wikipedia.org/wiki/Isolation_(database_systems)#Non-repeatable_reads)（Non-Repeatable Reads），例如：

  ~~~sql
  SELECT * FROM books WHERE id = 1;   						/* 时间顺序：1，事务： T1 */
  UPDATE books SET price = 110 WHERE id = 1; COMMIT;			/* 时间顺序：2，事务： T2 提交*/
  SELECT * FROM books WHERE id = 1; COMMIT;   				/* 时间顺序：3，事务： T1 提交*/
  ~~~

  这两次重复执行的查询结果就会不一样，原因是读已提交的隔离级别缺乏贯穿整个事务周期的读锁，无法禁止读取过的数据发生变化。

- **读未提交（Read Uncommitted）**：只对事务涉及的数据加上写锁，会一直持续到事务结束，但完全不加读锁。

  它存在[脏读问题](https://en.wikipedia.org/wiki/Isolation_(database_systems)#Dirty_reads)（Dirty Reads），例如：

  ```sql
  SELECT * FROM books WHERE id = 1;   						/* 时间顺序：1，事务： T1 */
  
  UPDATE books SET price = 90 WHERE id = 1;					/* 时间顺序：2，事务： T2 暂不提交*/
  
  SELECT * FROM books WHERE id = 1;		
  /* 时间顺序：3，事务： T1 */
  /* 如果这里是读已提交，那么 T1 因要施加读锁而一直被阻塞，直到 T2 提交了事务 */
  
  ROLLBACK;			  										/* 时间顺序：4，事务： T2 */
  ```

脏写问题（Dirty Write），即一个事务的没提交之前的修改，可以被另外一个事务的修改覆盖掉。脏写已经不单纯是隔离性上的问题了，它连事务的原子性都无法实现，所以在谈论隔离级别时，不会将它纳入讨论范围内。

除了都以锁来实现外，以上四种隔离级别还有另一个共同特点，就是幻读、不可重复读、脏读等问题都是由于一个事务在读数据过程中，受另外一个写数据的事务影响，而破坏了隔离性。针对这种「一个事务读 + 另一个事务写」的隔离问题，近年来有一种名为**[多版本并发控制](https://en.wikipedia.org/wiki/Multiversion_concurrency_control)（Multi-Version Concurrency Control，MVCC）**的无锁优化方案被业界广泛采用。**它的「无锁」是特指读取时不需要加锁。**  

 **MVCC 是典型的 CopyOnWrite 方案。**基本思路是对数据库的任何修改，都不会直接覆盖之前的数据，而是产生一个新版副本与老版本共存，以此达到读取时可以完全不加锁的目的。其中版本可以理解为每一行记录都存在两个不可见的字段：`CREATE_VERSION` 和 `DELETE_VERSION`，这两个字段记录的值都是事务 ID，事务 ID 是一个全局严格递增的数值，然后根据以下规则写入数据：

- 插入数据时：CREATE_VERSION 记录插入数据的事务 ID，DELETE_VERSION 为空。
- 删除数据时：DELETE_VERSION 记录删除数据的事务 ID，CREATE_VERSION 为空。
- 修改数据时：将修改数据视为「删除旧数据，插入新数据」的组合，即先将原有数据复制一份，原有数据的 DELETE_VERSION 记录修改数据的事务 ID，CREATE_VERSION 为空。复制出来的新数据的 CREATE_VERSION 记录修改数据的事务 ID，DELETE_VERSION 为空。

此时，如有另外一个事务要读取这些发生了变化的数据，将根据隔离级别来决定到底应该读取哪个版本的数据

- 可重复读：总是读取 CREATE_VERSION 小于或等于当前事务 ID 的记录，在这个前提下，如果数据仍有多个版本，则取最新（事务 ID 最大）的。
- 读已提交：总是取最新的 CREATE_VERSION 即可，即最近被 Commit 的那个版本的数据记录。

另外两个隔离级别都没有必要用到 MVCC，因为读未提交直接修改原始数据即可，其他事务查看数据的时候立刻可以看到，根本无须版本字段。可串行化本来的语义就是要阻塞其他事务的读取操作，而 MVCC 是做读取时无锁优化的，自然就不会放到一起用。MVCC 是只针对“读+写”场景的优化，如果是两个事务同时修改数据，即“写+写”的情况，那就没有多少优化的空间了，此时加锁几乎是唯一可行的解决方案。



此外，加锁的策略可以分为

- **“乐观加锁”（Optimistic Locking）**：认为事务之间数据存在竞争是偶然情况，没有竞争才是普遍情况，这样就不应该在一开始就加锁，而是应当在出现竞争时再找补救措施。
- **“悲观加锁”（Pessimistic Locking）**：认为不先做加锁再访问数据，就肯定会出现问题。

如果竞争剧烈的话，乐观锁反而更慢。




死锁检测就是发现有向图中存在着环。

![image-20240611114846041](./assets/image-20240611114846041.png)

检测到死锁后，数据库可以强制让其中某个事务回滚，释放掉锁，把环断开，死锁就解除了。

## MySQL 锁

MySQL 中的锁可以按照粒度分为**锁定整个表的表级锁**（table-level locking）和**锁定数据行的行级锁**（row-level locking）。InnoDB 存储引擎默认情况下采用行级锁。InnoDB 实现了以下两种类型的**行锁**：

- **共享锁**（S）
- **排他锁**（X）：写锁 + 范围锁

下面我们通过一个例子来说明 MySQL 中的锁机制。首先创建一个表：

~~~sql
create table t(
  id int auto_increment primary key,
  c1 int,
  c2 int,
  c3 int
);
create unique index idx_t_c1 on t(c1);
create index idx_t_c2 on t(c2);
~~~

由于 InnoDB 中的自动提交 autocommit 默认设置为 ON，即每个查询都被当做一个单独的事务自动执行。

设置共享锁：

~~~sql
select * from t where id = 1 for share;
~~~

设置排他锁：

~~~sql
select * from t where id = 1 for update;
~~~

`rollback` 或者 `commit` 会释放这些行锁（包括下面的表锁）。



实际上，InnoDB 表存在两种表级锁，一种是`LOCK TABLES`语句手动指定的表级锁，另一种是由 InnoDB 自动添加的意向锁。

~~~sql
lock tables t read; 		-- 为表 t 加上表级共享锁
lock tables ... write; 		-- 为表 t 加上表级排他锁
unlock tables;				-- 释放锁
~~~

如果一个表要添加表级锁，出于并发安全性考虑，那么它必须遍历每行，查看是否添加了行级锁，这种方式效率很低。因此，MySQL 引入了意向锁。**意向锁属于表级锁**，由 InnoDB 自动添加

- **意向共享锁**（IS）：事务在添加行级共享锁之前，必须先取得该表的 IS 锁。
- **意向排他锁**（IX）：事务在添加行级排他锁之前，必须先取得该表的 IX 锁。

这样，当事务 B 申请表级排他锁时，如果发现该表上存在意向共享锁，说明表中某些数据行上存在共享锁，事务 B 申请的表级排他锁会被阻塞。这样就避免了遍历整张表。

简单来说，**意向锁和表锁之间只有共享锁兼容，意向锁和意向锁之间都可以兼容**

![img](./assets/1401949-20210630171035477-675528309.png)



　**InnoDB 通过给索引上的记录加锁的方式实现行级锁**。具体来说，InnoDB 实现了三种行级排他锁：

- 记录锁（Record Lock）
- 间隙锁（Gap Lock）
- Next-key 锁（Next-key Lock）



**记录锁（Record Lock）是针对索引记录（index record）的锁定**。例如，`SELECT * FROM t WHERE id = 1 FOR UPDATE;`会阻止其他事务对表 t 中 id = 1 的数据执行插入、更新，以及删除操作。注意：

1. `id` 列必须为唯一索引列或主键列，否则会变成临键锁
2. 同时查询语句必须为精准匹配（`=`），不能为 `>`、`<`、`like`等，否则也会变成临键锁

在通过主键索引与唯一索引对数据行进行 UPDATE 操作时，也会对该行数据加`记录锁`：

```sql
-- id 列为主键列或唯一索引列
UPDATE SET age = 50 WHERE id = 1;
```



**间隙锁（Gap Lock）锁定的是索引记录之间的范围，或者第一条索引记录之前的范围，又或者最后一条索引记录之后的范围。**例如，`SELECT * FROM t WHERE c1 BETWEEN 1 and 10 FOR UPDATE;`会阻止其他事务将 1 到 10 之间的任何值插入到 c1 字段中，即使该列不存在这样的数据；因为这些值都会被锁定。间隙锁只有在事务隔离级别 RR 中才会生效，它是用于避免**幻读**（Phantom Read）问题。

- 唯一索引只有锁住多条记录，或者一条不存在的记录的时候，才会产生间隙锁
- 普通索引不管是锁住单条，还是多条记录，都会产生间隙锁





**Next-Key** 可以理解为一种特殊的间隙锁。间隙锁是左开右开的区间，而临键锁是左开右闭。假设有如下表：

|  id  | age  |  name  |
| :--: | :--: | :----: |
|  1   |  10  |  Lee   |
|  3   |  24  | Soraka |
|  5   |  32  |  Zed   |
|  7   |  45  | Talon  |

该表中 `age` 列潜在的临键锁有：(-∞, 10]、(10, 24]、(24, 32]、(32, 45]、(45, +∞]

## 原子性和持久性

由于写入中间状态与崩溃都是无法避免的，为了保证原子性和持久性，就只能在崩溃后采取恢复的补救措施——「崩溃恢复」。

### Commit Logging

通过「Commit Logging」（提交日志）来实现事务的原子性和持久性是当今的主流方案，基本思路是

1. 将修改数据这个操作所需的全部信息，包括修改什么数据、数据位于哪个磁盘块中、从什么值改成什么值，等等，以日志的形式（在文件末尾顺序追加）记录到磁盘中
2. 写入完成后，会追加「提交记录」（Commit Record），表示事务成功提交。
3. 然后根据日志上的信息对真正的数据进行修改
4. 修改完成后，再往日志中加入一条「结束记录」（End Record），表示事务已完成持久化。

日志一旦成功写入 Commit Record，那整个事务就是成功的，即使真正修改数据时崩溃了，重启后根据已经写入磁盘的日志信息恢复现场即可（持久化的双写策略），这保证了持久性；其次，如果日志没有成功写入 Commit Record 就发生崩溃，那整个事务就是失败的，系统重启后会看到没有 Commit Record 的日志，那将这部分日志标记为回滚状态即可，这保证了原子性。

> 除了Commit Logging，还有另外一种称为“[Shadow Paging](https://en.wikipedia.org/wiki/Shadow_paging)”的事务实现机制。大体思路是当写入数据时，先复制一份副本，保留原数据，修改副本数据。当事务成功提交后，去修改引用数据的指针，现代磁盘的写操作可以保证这个修改指针的操作是原子性的。
>
> Shadow Paging 实现事务要比 Commit Logging 更加简单，但涉及隔离性与并发锁时，Shadow Paging 实现的事务并发能力就相对有限，因此在高性能的数据库中应用不多。

Commit Logging 存在一个先天缺陷，所有对数据的真实修改都必须发生在事务提交以后，即日志写入了 Commit Record 之后，即使之前你的磁盘 I/O 足够空闲。如果某个事务修改的数据量非常庞大，那么会占用大量的内存缓冲区。

### Write-Ahead Logging

> FORCE/STEAL 留在最后再看
>
> 部分内容与 Commit Logging 重复
>
> 

ARIES 提出了「Write-Ahead Logging」的日志改进方案，它将写入数据的时机，按照事务提交时点为界，划分为 FORCE 和 STEAL 两类情况

- **FORCE**：已经提交的事务必须强制写入磁盘。注：这里的事务指的是数据本身，而不是日志。
- **NO FORCE**：已经提交的事务可以保留在内存里。
- **STEAL**：未提交的事务也能写入，如果事务需要回滚，再更改磁盘上的数据。
- **NO STEAL**：未提交的事务不能写入磁盘，只能在内存中操作，等到事务提交完，再把数据一次性写入

可以两两组合出四种情况：

![image-20240328173035634](./assets/image-20240328173035634.png)

从优化磁盘 I/O 的角度看，NO-FORCE 加 STEAL 组合的 性能无疑是最高的，但复杂度无疑也是最高的。这也是业界的主流方案。显然Commit Logging 是 NO-FORCE，NO-STEAL 的；而 Write-Ahead Logging 是 NO-FORCE，STEAL 



Write-Ahead 算法：

1. 先在内存中提交事务
2. 将日志（Write-aheadLog）写入到磁盘上
3. 后台任务把内存中的数据异步刷到磁盘

这里有个重要前提，即日志中的信息是可以等价描述原数据的。

「内存操作数据 + Write-Ahead Log」这种思想非常普遍。具体到 InnoDB 中，Write-Ahead Log 是 Redo Log，而且连 Redo Log 的写入也是异步的。参数 `innodb_flush_log_at_trx_commit` 决定 Redo Log 的刷盘策略：

- 0（默认值）：每秒刷一次磁盘，把 RedoLog Buffer 中的数据刷到 Redo Log
- 1：每提交一个事务，就刷一次磁盘
- 2：不刷盘。然后根据参数 `innodb_flush_log_at_timeout` 设置的值决定刷盘频率。

![image-20240611234716178](./assets/image-20240611234716178.png) 



Redo Log 可以理解为一个固定大小的文件（实际上是一组文件），循环使用，写到尾部之后，回到头部覆写。之所以能覆写，是因为一旦 Page 数据刷到磁盘上，对应的日志数据就没有存在的必要了。

![image-20240611235023967](./assets/image-20240611235023967.png)

其中，**LSN（Log Sequence Number）**是逻辑上日志按照时间顺序从小到大的编号。

下面考虑如何记录修改，有以下三种方案：

1. 记录原始的 SQL 语句
2. 记录每张表的每条记录中的修改前的值、修改后的值，类似（表，行，修改前的值，修改后的值）四元组。
3. 记录 Page 中每个被修改的字节数据，类似（Page ID,offsetl,lenl,改之前的值，改之后的值）四元组

前两种是逻辑记法，而后两种是物理记法。逻辑日志在宕机后不好恢复；物理日志又太大，这是因为一条逻辑日志就可能对应多条物理日志。

因此 MySQL 采用 Physiological Logging，类似于（Page ID，Record Offset，(Filed 1, Value 1) … (Filed i, Value i) … ) 。其中，Page ID、Record Offset 的设计源于物理日志。(Field1,Value1) 的设计来源于逻辑日志，含义是需要修改的字段以及对应的值。注意，一个事务若涉及多个 Page，那么每一个 Page 都唯一地对应一段日志。



如果将 Block 写入磁盘过程中宕机了，那么写入成功的字节数是多少呢？这取决于操作系统以及硬件给出的原子性保障。如果没有提供原子性，那么写入的字节数是任意的。可以通过 checksum 来判断一个 Log Block 是否完整。

如果在写入 16KB Data Page 时，系统宕机了。即使有 Physiological Logging，也无法恢复 Page。这是因为该 Logging 仅记录了逻辑修改。好在有两个解决办法：

- 让硬件支持 16KB 写入的原子性。
- Double write。把16KB写入到一个临时的磁盘位置，写入成功后再拷贝到目标磁盘位置。这样，即使目标磁盘位置的16KB因为宕机被损坏了，还可以用备份去恢复。

RedoLog Block 的详细结构如下：

![image-20240612000733496](./assets/image-20240612000733496.png)

- Block No：每个 Block 的唯一编号，可以由 LSN 换算得到。
- Date Len：该 Block 中实际日志数据的大小
- First Rec Group：该 Block 中第一条日志的起始位置，可能因为上一条日志很大，上一个 Block 没有存下，日志的部分数据到了当前的 Block。极端情况 First Rec Group = Data Len
- Checkpoint No：当前 Block 进行 Check point 时对应的 LSN



下面我们通过一个示例来说明事务和对应的 Redo Log 之间的关系：

~~~sql
start transaction
    update 表1某行记录
    delete 表1某行记录
    insert 表2某行记录
commit
~~~

![image-20240612001952201](./assets/image-20240612001952201.png)

可以看出，由于异步写入，同一个事务所涉及要修改的 Page 下的多个日志部分可能是不连续的。

注意到，已提交事务的日志和未提交事务的日志是混在一起的。当恢复时，会把 Redo Log **全部**重放一遍。完成后，把未完成的事务找出来后，逐一利用 Undo Log 回滚。

回滚思路用这个例子来说明。如下图所示，客户端提交了 Rollback，数据库直接执行三个相反效果的 SQL 语句，然后 Commit，即将 Rollback 转换为 Commit。这是逻辑层面上的回滚，而不是物理层面上的直接回滚。

![image-20240612003029126](./assets/image-20240612003029126.png)

对应到 Redo Log 的回滚，就是重新添加日志。这种逆向操作的 SQL 语句对应到RedoLog 里面，叫作 Compensation Log Record，会和正常操作的 SQL的 Log 区分开来。



下面介绍 ARIES 恢复算法。如下图所示，有 T0～T5 共 6 个事务，每个事务所在的线段代表了在 Redo Log 中的起始和终止位置。发生宕机时，T0、T1、T2 已经完成，T3、T4、T5 还在进行中，所以回滚的时候，要回滚 T3、T4、T5。

![image-20240612003636927](./assets/image-20240612003636927.png)



ARIES 算法首先是分析阶段，它要解决两个核心问题：

1. 确定哪些数据页是脏页，即如何找出从 Checkpoint 到 Crash 之前，所有未刷盘的Page
2. 确定哪些事务未提交

**Sharp Checkpoint** 是每隔一段时间对内存中的数据拍一个快照，然后「一次性」刷到磁盘上。但它的应用场景很苛刻，因为数据库还在不断地接受客户端的请求，这些脏页一直在更新，除非阻塞客户端的请求。在工业实践中，更多使用 **Fuzzy Checkpoint**。它在内存中维护了两张表：

1. 活跃事务表：当前所有未提交事务的集合

   ![image-20240612005217091](./assets/image-20240612005217091.png)

   这里 lastLSN 是该事务所产生的最后一个日志的 LSN。

2. 脏页表：当前所有未刷到磁盘上的 Page 的集合（包括了已提交的事务和未提交的事务）

   ![image-20240612005327904](./assets/image-20240612005327904.png)

   这里 recoveryLSN 是最早导致该 Page 为脏页的日志的 LSN

而 Fuzzy Checkpoint 就是对这两个表做了一个 Checkpoint，这相对于备份 Redo Log 轻量很多，所等待的备份时间是可以接受的。

基于这两张表以及上例，我们来说明如何解决问题（2）

1. 在最近的一次 Checkpoint2时候，未提交事务集合是{T2,T3}，从此处开始，遍历Redo Log 到末尾。
2. 在遍历的过程中，首先遇到了 T2 的结束标识，把 T2 从集合中移除，剩下{T3}
3. 之后遇到了事务T4的开始标识，把 T4 加入集合，集合变为{T3,T4}
4. 之后遇到了事务T5的开始标识，把 T5 加入集合，集合变为{T3,T4,T5}
5. 最终直到末尾，没有遇到 {T3,T4,T5} 的结束标识，所以未提交事务是{T3,T4,T5}

解决问题（1）。假设在 Checkpoint2 的时候，脏页的集合是 {P1,P2} 。从 Checkpoint 开始，一直遍历到 Redo Log 末尾，一旦遇到不在当前集合中的 Page，就把它加入脏页集合。



接着进入 Redo 阶段，假设最后求出来的脏页集合是 {P1,P2,P3,P4,P5}。取集合中所有脏页的 recoveryLSN 的最小值，得到 firstLSN。从 firstLSN 遍历 Redo Log 到末尾，把每条 Redo Log 对应的 Page 全部重刷一次磁盘。

通过 pageLSN 保证重放时的幂等。这个 LSN 记录的是这个 Page 刷盘时最后一次修改它的日志对应的 LSN，它直接存放在 Page 中。如果重放日志的时候，日志的 LSN <= pag eLSN，则过此条日志。

Redo 完成后，就保证了所有的脏页都成功地写入到了磁盘，干净页也可能重新写入了一次。



最后就是 Undo 阶段。在分析阶段，我们已经找出了未提交事务集合{T3,T4,T5}。从最后一条日志逆向遍历，因为每条日志都有一个 prevLSN 字段，所以可以沿着 T3、T4、T5 各自的日志链一直回溯。每遇到一条属于 T3、T4、T5 的 Log，就生成一条逆向的 SQL 语句来执行，其执行对应的 Redo Log 是 Compensation Log Record。

在进行 Undo 操作的时候，还可能会遇到「嵌套回滚」问题，回滚到一半，宕机，重启，再回滚。通过 CLR 中的 UndoNxtLSN 字段来解决这个问题，它记录对应被回滚日志的前一条日志的 LSN。如下图所示，假设要回滚一个未提交的事务T，其有三条日志 LSN 分别为600、900、1000。第一次宕机重启，首先对 LSN=1000 进行回滚，生成对应的 LSN=1200 的日志，它的 UndoNxtLSN=900。这样当再一次宕机重启时，遇到 LSN=1200 的 CLR时，会忽略这条日志，直接定位到 LSN=900 的日志，为其生成对应的 CLR 日志 LSN=1600，然后继续回滚

![image-20240612021411756](./assets/image-20240612021411756.png)

## Binlog 

Redo Log 记录日志， Binlog 也记录日志。但 Redo Log 和 Undo Log 是 InnoDB 引擎层面的概念，而 Binlog 是 MySQL 层面的，其主要作用用于主从复制，或者实现数据监听。

Binlog 全局只有一份，每个事务都要串行地写入，这就要求每个事务在写 Binlog 之前要拿一把锁。这在性能上存在很大问题。MySQL 5.6 中引入了 Group Commit 来解决这个问题，思想就是 HTTP 1.1 中的 pipeline 机制。  

一个事务的提交既要写 Binlog，也要写 Redo Log，通过内部分布式事务管理器 XA 来解决它们两的数据一致性问题。



MySQL的三种主从复制方式

![image-20240612023517714](./assets/image-20240612023517714.png)

另外，还有一个参数 `rpl_semi_sync_master_wait_slave_count`可以设置在半同步复制模式下，需要等待几个（默认是 1 个） Slave 的 ACK，才认为事务提交成功。

