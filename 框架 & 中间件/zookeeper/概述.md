# 概述

Zookeeper 的设计目标是将复杂且容易出错的任务协作逻辑封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用，用户在这些接口的基础上，可以构建应用自己的原语。基于它可以实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。

ZooKeeper 所操作的对象是类似于文件系统的层级树状结构，每个节点被称为 znode

![image-20240617124412011](./assets/image-20240617124412011.png)

注：框里的内容表示该 znode 所存储的数据。

znode 里保存的是字节序列，它并不提供序列化的能力。序列化这项工作由应用来处理。ZooKeeper 的 API 暴露了以下方法：

- `create /path data`：创建一个路径为 /path 的 znode，并包含数据 data
- `delete /path`：删除 /path 节点，它只能删除不包含子节点的节点
- `exists /path`：检查是否存在 /path 节点。
- `setData /path data`：将 /path 节点的数据设置为data。
- `getData /path`：返回 /path 节点的数据
- `getChildren /path`：返回 /path 节点下所有子节点

注意，zookeeper 并不支持部分读写 znode 下的数据

znode 的类型有：

- 持久（persistent）节点：该节点只能被 delete API 删除

- 临时（ephemeral）节点：在以下两种情况下将会被删除

  - 当创建该 znode 的客户端的会话关闭时
  - 当某个客户端（不一定是创建者）主动删除该节点

  不允许临时节点拥有子节点

- 有序（sequential）节点：一个持久节点可以设置为有序的。有序 znode 节点会被分配唯一个单调递增的整数，该整数会追加到路径后。此外，递增整数是针对该目录（父节点）下的，而不是全局的。

- 临时有序（ephemeral_sequential）



为了避免客户端轮询操作所带来的性能消耗（见下图），我们可以使用通知（notification）机制。

![image-20240617130859541](./assets/image-20240617130859541.png)

客户端向 ZooKeeper 注册需要监听的 znode。监视点是一个单次触发的操作，为了在同一个节点上多次监听，客户端必须在每次通知后设置一个新的监视点。

![image-20240617131043193](./assets/image-20240617131043193.png)

关于监控的更多细节，我们留在 Watcher 小节中讲解。



每一个 znode 都有一个版本号，它随着每次数据变化而自增。在调用 setData 或者 delete 时，需要传入一个版本号。如果该版本号与服务器上的版本号不一致，那么调用失败。

![image-20240617140822176](./assets/image-20240617140822176.png)



ZooKeeper 架构总览

![image-20240617140940959](./assets/image-20240617140940959.png)

这里服务端采用无主复制中的基于 Quorum 的数据冗余机制。



ZooKeeper 的安装（独立模式）

1. 从 http://zookeeper.apache.org 中下载压缩包
2. 解压 `tar -xvzf zookeeper-3.4.5.tar.gz`
3. 重命名配置文件：`mv conf/zoo_sample.cfg conf/zoo.cfg`
4. 启动服务器：`bin/zkServer.sh start`

客户端可以通过 `bin/zkCli.sh` 脚本工具来与服务端进行交互。下面介绍如何搭建一个 ZooKeeper 集群（仲裁模式）

1. 每一个节点的配置文件如下：

   ~~~shell
   tickTime=2000
   initLimit=10
   syncLimit=5
   dataDir=./data
   clientPort=2181
   server.1=127.0.0.1:2222:2223
   server.2=127.0.0.1:3333:3334
   server.3=127.0.0.1:4444:4445
   ~~~

   这里解释一下：server.n=ip:port1:port2：

   - n 是 Zookeeper 服务节点的编号
   - port1：仲裁通信的端口
   - port2：选主的端口

2. 在每个节点的 dataDir 目录下创建 myid 文件，里面只有一个整数，表示该节点的编号

使用 zkCli.sh 来访问集群：

~~~shell
/bin/zkCli.sh -server 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183
~~~





客户端的会话生命周期：

![image-20240617141612781](./assets/image-20240617141612781.png)

如果一个客户端与服务器因超时而断开连接，客户端仍然保持 CONNECTING 状态，直到分区问题修复，客户端才能从服务端那里获知会话已经 CLOSED。发生这种行为是因为 ZooKeeper 集合对会话超时负责，而不是由客户端负责

如果经过时间 t 之后，服务接收不到这个会话的任何消息，服务就会声明会话过期。而在客户端侧，如果经过 t/3 的时间后未收到任何消息，客户端将向服务器发送心跳消息。在经过 2t/3 时间后仍会收到消息，ZooKeeper 客户端开始寻找其他的服务器，而此时它还有 t/3 时间去寻找

当客户端尝试连接到一个不同的服务器时，这个服务器的 ZooKeeper 状态要比最后连接的服务器的ZooKeeper状态新。也就是说，如果一个客户端在位置 i 观察到一个更新，它就不能连接到观察到 i'<i 的服务器上。这点可以通过 zkid 事务标识符来实现

![image-20240617142558528](./assets/image-20240617142558528.png)



## 案例：锁

假设有一个应用由 n 个进程组成，每个进程 p 尝试创建路径为 /lock 的临时 znode（避免进程崩溃而释放不了锁，从而形成死锁）。如果进程 p 成功创建了znode，就表示它获得了锁，并可以继续执行其临界区域的代码。其他进程因 znode 存在而创建失败，它们监听 /lock 的变化，并在检测到 /lock 删除时，再次尝试创建节点来获得锁。

## 案例：主-从模式

主-从模式的模型中包括三个角色

- 主节点：负责监视新的从节点，分配任务给可用的从节点。
- 从节点：通过系统注册自己，等待执行任务
- 客户端：创建新任务并等待系统的响应



1. 利用锁的思路，创建一个节点 /master，来完成选举主节点以及自动故障转移的工作。
2. 创建三个持久化节点：/workers、/tasks 和 /assign
3. 主节点监控  /tasks 节点的变化情况
4. 从节点通过在 /workers 子节点下创建临时性的 znode，并使用主机名来标识自己
5. 从节点需要创建并监控  /assing/worker1.example.com ，来接受任务的分配
6. 客户端通过创建并监控有序节点 /tasks/task- ，来等待获取任务结果
7. 主节点会监控到 /tasks 发生变化，然后从 /workers 获取可用节点，然后在 /assing/worker1.example.com 创建节点
8. 从节点接收到新任务分配的通知，然后执行任务。
9. 执行完任务后，从节点创建 /tasks/task-0000000000/status ，客户端会监听到这一事件，获取该节点的数据

