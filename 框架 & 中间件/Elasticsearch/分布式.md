# 分布式

[TOC]

## 分布式

ES 使用了 Quorum 机制来避免脑裂，在进行选主的时候，需要超过半数 Master 候选节点参与选主才行。

在为系统增加存储空间时，我们只需要为集群添加一个数据节点即可，ES 会为我们自动完成剩下的工作 。下面看一个示例副本保持的示例：

1. 初始状态

![一节点主分片.png](assets/129871fc602b46bf8d122309148cf988tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

2. 添加一个节点，分片副本在新节点上创建

![二节点主副分片.png](assets/779aba049816440a9fdc00daa17ad924tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

为了保证数据的可靠性，在发生分片故障的时候，ES 是会自动完成数据迁移的：

1. 系统会先在其他节点上恢复主分片
2. 然后再从其他节点上恢复足够多的副本分片。



1. 主节点下线

![Master下线.png](assets/9f9b7b6773844845a779ca9f47ad3b83tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

2. 选取主分片

![主分片迁移.png](assets/9f426eb118be40da8dbcf80317eac437tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

3. 生成副本

![生成副本.png](assets/a8d3f74d3a2a46c0973845385c3880fctplv-k3u1fbpfcp-jj-mark1512000q75.webp)





当一个索引的主分片数量比数据节点数量多的时候，如果加入新的数据节点，系统会自动将这些分片分配到其他机器上，以达到一个分片平衡的状态。

![主分片迁移.png](assets/40d5337cd78241efacecb792f191ba61tplv-k3u1fbpfcp-jj-mark1512000q75.webp)







ES 的索引是由多个分片组成的，每个分片尽量均匀地分散到各个节点上存储，以避免出现数据倾斜。ES 的数据路由算法是根据 routing key 来确定 Shard ID 的过程，计算公式如下：
$$
shard\_number = hash(\_routing) \% numer\_of\_primary\_shards
$$
**默认的情况下 routing key 为文档 ID**。可以在请求中指定 routing key

~~~bash
PUT books/_doc/doc_id?routing=routing_key
{
    "name": "java",
    "id": "book_id"
}
~~~



整体来说，ES 的搜索过程可以分为两个阶段：

1. Query 阶段：Query 阶段会根据搜索条件遍历每个分片中的数据，返回符合条件的前 N 条数据的 ID 和排序值。然后在协调节点中对所有分片的数据进行排序，获取前 N 条数据的 ID。

   ![Query流程.png](assets/55e88f3200674e80bff577e4f6e691f1tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

   1. 客户端发起 search 请求到 Node1
   2. 协调节点 Node1 将查询请求转发到索引的每个主分片或者副分片中，每个分片执行本地查询并且将查询结果打分排序
   3. 每个分片将查询结果返回到 Node1（协调节点）中，Node1 对所有结果进行排序，并且把排序后结果放到一个全局的排序列表中。

   需要注意的是，在协调节点转发搜索请求的时候，如果有 N 个 Shard 位于同一个节点时，并不会合并这些请求，而是发起 N 次请求。

2. Fetch 阶段：根据 Query 阶段所产生的全局排序列表，确定需要取回的文档 ID 列表（此时仅有文档的部分信息，例如文档的 _id 和得分），通过 multi get 的方式到对应的分片上获取文档数据。

   ![Fetch流程.png](assets/1e2405a573c647299522ee9f5ce74b71tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

   1. 协调节点（Node1）确定哪些文档需要获取，然后向相关节点发起 multi get 请求；
   2. 分片所在节点读取文档数据，并且进行 _source 字段过滤、处理高亮参数等，然后把处理后的文档数据返回给协调节点；
   3. 协调节点等待所有数据被取回后返回给客户端。





从 Query 阶段我们知道，文档的评分是在各自的 Shard 上获取的，因为 ES 的每个 Shard 就是 Lucene 的一个索引，所以**每个分片都是基于自己的数据进行相关性算分的**，即分片的相关性算分都是独立的。这样文档的算分只是基于系统中部分数据来计算的，从而会导致打分偏离的情况。有三种解决方式：

1. 如果索引的数据不多的情况下，可以设置主分片数为 1
2. 索引数据量大的情况下，需要保证数据均匀地分布在各个分片中（推荐）
3. 使用 DFS Query Then Fetch， 在 URL 参数中指定：_search?search_type=dfs_query_then_fetch。这样设定之后，系统先会把每个分片的词频和文档频率的数据汇总到协调节点进行处理，然后再进行相关性算分。这样的话会消耗更多的 CPU 和内存资源，效率低下！





数据在主分片写入成功后，再将数据分发到副分片进行存储：

![写入单个文档流程.png](assets/4e0b142686e84893be26ac0b1868050atplv-k3u1fbpfcp-jj-mark1512000q75.webp)

1. 客户端的请求到达 Node1，Node1 根据 routing key 来计算得出文档应该被保存到哪个分片（这里是分片 0），并且从集群状态的内容路由表信息中，获取分片 0 所在的节点为 Node2；
2. Node1 将请求转发到 Node2 处理，Node2 执行写入操作；
3. 如果写入成功，Node2 将写请求并发转发到 Node1 和 Node3，并且执行副本写入操作；
4. 当所有副分片都写入成功后，Node2 会向协调节点（这里的 Node1）返回写入成功的信息，最后协调节点向客户端返回成功





## 分页

Query + Fetch 的方式，会产生一些问题：

- 每个分片上都要取回 from + size 个文档（不是 from 到 size，而是 from + size）；
- 协调节点需要处理 shard_amount * ( from + size ) 个文档。

这会有深分页问题，举个例子：

![深度分页的原因.png](assets/d142290d512b4cdb8680325a27f41f6ctplv-k3u1fbpfcp-jj-mark1512000q75.webp)

当使用 from = 90 和 size = 10 进行分页的时候，ES 会先从每个分片中分别获取 100 个文档，然后把这 300 个文档再汇聚到协调节点中进行排序，最后选出排序后的前 100 个文档，返回第 90 到 99 的文档。



ES 为解决分页的需求提供了 3 种 API：

- **from + size**
- **search after**：只能一页一页地往下翻，不支持跳转到指定页数。
- **scroll API**：会创建数据快照，无法检索新写入的数据，适合对结果集进行遍历的时候使用。



在我们检索数据时，系统会对数据按照相关性算分进行排序，然后默认返回前 10 条数据。我们可以使用 from + size 来指定获取哪些数据

~~~bash
# 简单的分页操作
GET books/_search
{
  "from": 0, # 指定开始位置
  "size": 10, # 指定获取文档个数
  "query": {
    "match_all": {}
  }
}
~~~

将 from 设置大于 10000 或者 size 设置大于 10001 的时候，这个查询将会报错。因为我们要获取的数据集合太大了，系统拒绝了我们的请求。可以使用 "index.max_result_window" 配置项设置这个上限：

~~~bash
PUT books/_settings
{
  "index": {
    "max_result_window": 20000
  }
}
~~~





使用 search after API：

1. 在 sort 中指定需要排序的字段，并且保证其值的唯一性
2. 在下一次查询时，带上返回结果中最后一个文档的 sort 值进行访问

~~~bash
# 第一次调用 search after
POST books/_search
{
  "size": 2,
  "query": { "match_all": {} },
  "sort": [
    { "price": "desc" },
    { "_id": "asc" }
  ]
}

# 返回结果
"hits" : [
  {
    "_id" : "6",
    "_source" : {
      "book_id" : "4ee82467",
      "price" : 20.9
    },
    "sort" : [20.9, "6"]
  },
  {
    "_id" : "1",
    "_source" : {
      "book_id" : "4ee82462",
      "price" : 19.9
    },
    "sort" : [19.9, "1"]
  }
]

# 第二次调用 search after
POST books/_search
{
  "size": 2,
  "query": {
    "match_all": {}
  },
  "search_after":[19.9, "1"], # 设置为上次返回结果中最后一个文档的 sort 值
  "sort": [
    { "price": "desc" },
    { "_id": "asc" }
  ]
}
~~~

因为有了唯一的排序值做保证，所以每个分片只需要返回比 sort 中排序值大的 size 个数据即可。而 from + size 的方式因为没有唯一排序值，所以没法保证每个分片上的排序就是全局的排序，必须把每个分片的 from + size 个数据汇总到协调节点进行排序处理，导致出现了深分页的问题。



使用 scroll API。scroll API 会创建数据快照，后续的访问将会基于这个快照来进行，所以无法检索新写入的数据。这和在SQL中使用游标的方式非常相似。通过 size 参数指定每次 scroll API 所获取的文档个数

~~~bash
# 第一次使用 scroll API
# 需要初始化 scroll 搜索并且创建快照
# scroll 参数告诉 Elasticsearch 将 “search context” 保存多久
POST books/_search?scroll=10m
{
  "query": {
    "match_all": {}
  },
  "sort": { "price": "desc" }, 
  "size": 2
}

# 结果
{
  "_scroll_id" : "FGluY2x1ZGVfY29udGV4dF9......==",
  "hits" : {
    "hits" : [
      {
        "_id" : "6",
        "_source" : {
          "book_id" : "4ee82467",
          "price" : 20.9
        }
      },
      ......
    ]
  }
}

# 我们把 _scroll_id 递给 scroll API ，用来取回下一批结果
POST /_search/scroll                                                    
{
  "scroll" : "1m"
  "scroll_id" : "FGluY2x1ZGVfY29udGV4dF9......==" 
}
~~~





在 ES 7.10 中引入了 Point In Time 后，scroll API 就不建议被使用了。PIT 是一个轻量级的数据状态视图，用户可以利用这个视图反复查询某个索引，仿佛这个索引的数据集停留在某个时间点上。也就是说，在创建 PIT 之后更新的数据是无法被检索到的。

~~~bash
# 使用 pit API 获取一个 PID ID
POST /books/_pit?keep_alive=20m

# 结果
{
  "id": "46ToAwMDaWR5BXV1aWQy......=="
}
~~~

建议使用 PIT + search after 代替 scroll API

~~~bash
POST _search
{
  "size": 2,
  "query": { "match_all": {} },
  "pit": {
    "id":  "46ToAwMDaWR5BXV1aWQy......==", # 添加 PIT id
    "keep_alive": "5m" # 视图的有效时长
  },
  "sort": [
    { "price": "desc" } # 按价格倒序排序
  ]
}

# 第二次调用 search after，因为使用了 PIT，这个时候搜索不需要指定 index 了。
POST _search
{
  "size": 2,
  "query": {
    "match_all": {}
  },
  "pit": {
    "id":  "46ToAwMDaWR5BXV1aWQy......==", # 添加 PIT id
    "keep_alive": "5m" # 视图的有效时长
  },
  "search_after": [19.9, 8589934592], # 上次结果中最后一个文档的 sort 值
  "sort": [
    { "price": "desc" }
  ]
}
~~~

## 聚合

Max 聚合请求先达到协调节点，协调节点会将请求转发到所有保存主分片的节点进行处理，然后每个节点在本地分片中求出数据的最大值返回给协调节点，协调节点在各个分片的最大值中得出最大值返回给客户端。上述 Max 聚合的工作原理是不会产生聚合结果不准确的问题。

![Max的执行流程.png](assets/7c3516359a244c16a61ff0ac51488af2tplv-k3u1fbpfcp-jj-mark1512000q75.webp)



协调节点会从每个分片的 top n 数据中最终排序出 top n，但每个分片的 top n 并不一定是全量数据的 top n。

![Terms不准确示例.png](assets/bf5d1cd6e9e847568d5adc84814dc961tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

注这里的 key 是在Mapping中自定义的ID字段，而不是 ES 给文档分配的ID。

对于总量数据来说，数据排序为：A(20)、B(16)、D(8)、C(7)，top 3 为：A(20)、B(16)、**D(8)** 。而聚合得出的错误 top 3 为：A(20)、B(16)、**C(5)** 。因为在 P0 分片的 top 3 丢掉了 D(4), 而 P1 分片中返回 top 3 中丢掉了 C(2)，而丢掉的这部分数据恰好会影响最终的结果。



**doc_count_error_upper_bound**：在 P0 中数据返回了 A(10)、B(8)、C(5) ，因为返回的分组里面数据最小的是 C 分组的值 5，所以遗漏的可能的最大值就是 5。同理， P1 中的可能最大值为 4。所以 doc_count_error_upper_bound = 5 + 4 = 9。

**sum_other_doc_count** 计算就比较好理解了，将文档总数减去返回的统计总数。所以 sum_other_doc_count = 51 - 41 = 10。



解决方案：

1. 数据不要分片存储

2. 每个分片返回足够多的分组：例如查询结果要返回 top 3，我们在每个分片上返回 top 20

   ~~~bash
   GET /my_index/_search
   {
     "aggs": {
       "products": {
         "terms": {
           "field": "product",
           "size": 5,
           "shard_size": 10
         }
       }
     }
   }
   ~~~





   

![三角关系.png](assets/a39f8f96207a4682a8a7689d4621a598tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

而要保证实时性和准确度的话，只能对有限的数据进行计算。而硬件资源只能决定有限数据集合的上限。

## 集群

一个节点承担多种角色，这让可以很大程度上节省机器资源。但当存储数据很多时，推荐每个节点担任一种角色。这样每个节点职责分离的好处是：

- 每种类型的节点其对硬件的需求都不一样，可以按需分配。

  | 节点类型                          | 职责                            | 配置                                              |
  | --------------------------------- | ------------------------------- | ------------------------------------------------- |
  | Master 候选节点                   | Master 负责了整个集群的状态管理 | 低配置的 CPU、内存、磁盘                          |
  | 数据节点（Data Node）             | 负责数据存储和读取              | 高配置的 CPU、内存、磁盘                          |
  | 协调节点（Coordinating Nod**e**） | 负责请求的分发，搜索结果的整合  | 高配置的 CPU、中等配置的内存、低配置的磁盘        |
  | 预处理节点（Ingest Node）         | 负责处理数据、数据转换          | 高配置的 CPU、中等配置的内存、低配置的磁盘        |
  | Warm & Hot Node                   | 存储冷、热数据的数据节点        | Hot 类型的节点，都是高配配置，Warm 都是中低配即可 |

- 可以动态地对某种节点进行扩容



我们在协调节点前加了一个 LB（负载均衡器）来对请求负载到各个协调节点。

![基本架构.png](assets/ec89865b2004416d8559248c2939055ftplv-k3u1fbpfcp-jj-mark1512000q75.webp)

读写分离架构

![读写分离.png](assets/3f938e3fe54948228d23ac8070a294b1tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

多机房异地多活

![异地多活.png](assets/6cd2d0b1812643438339321bba2d6009tplv-k3u1fbpfcp-jj-mark1512000q75.webp)





应用分为两种类型：

1. 日志类业务：数据增长快，但是这些数据是基于时序存储，可能一年后就不会再查询了。
2. 搜索类业务：业务数据增长缓慢，但是数据可能一直都被查询，可能需要做一些复杂查询操作。



下面是[阿里云文档](https://link.juejin.cn/?target=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F72660.html)给出的建议：

- **日志类业务：** 单节点磁盘最大容量 = 单节点内存大小（GB）* 50，即内存与磁盘的比例为 1 : 50。
- **搜索性能要求高的场景**：单节点磁盘最大容量 = 单节点内存大小（GB）* 10，即内存与磁盘的比例为 1 : 10。
- **正常情况**：单节点磁盘最大容量 = 单节点内存大小（GB）* 30，即内存与磁盘的比例为 1 : 30。



Elastic Stack 的安全层级如下图

![elastic-security-overview.png](assets/f8b59a365903409da87289a428938fa1tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

为保证集群的安全，我们需要做以下三件事情：

- 集群访问进行身份认证
- 节点间通信加密，通过节点间通信数据的加密
- 外部使用 https 访问集群
- 对使用集群的用户进行身份认证和用户鉴权。



## ELK

使用 ELK 搭建一个简单的日志系统，它包括

- **Filebeat**，轻量级的日志收集处理工具
- **Logstash**，主要是用来日志的搜集、分析、过滤日志的工具
- **Elasticsearch**，是个开源分布式搜索引擎，提供搜集、分析、存储数据三大功能
- **Kibana**，提供的日志分析友好的 Web 界面，可以帮助汇总、分析和搜索重要数据日志



ELK 架构一

![架构一.png](assets/a3cbb9434a384d3b8031e44aa3428116tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

ELK 架构二（把采集日志的组件替换成了轻量级的 Filebeat）

![架构二.png](assets/aaab92ad8c77481f86e9493472b7fdf7tplv-k3u1fbpfcp-jj-mark1512000q75.webp)



ELK 架构三（为了加强系统的稳定性，这里加入了 Kafka 组件，起到削峰、异步操作、日志重放的作用，可以避免数据丢失）

![架构三.png](assets/bf6d0bb15c65473fb05182d15da2b11btplv-k3u1fbpfcp-jj-mark1512000q75.webp)



### 搭建 Kafka

~~~bash
# 使用 wget 下载
wget --no-check-certificate https://dlcdn.apache.org/kafka/3.1.0/kafka_2.13-3.1.0.tgz

# 解压
tar xvf kafka_2.13-3.1.0.tgz

# 重命名
mv kafka_2.13-3.1.0 kafka
~~~



~~~bash
# 配置 config/server.properties ：
listeners=PLAINTEXT://localhost:9092
~~~



编写启动脚本

~~~bash
function kill_all_app_by_name()
{
        pid=`ps -ef | grep $1 | grep -v "grep" | awk -F" " '{print $2}'`
        for i in $pid; do
                kill -9 $i
        done
}

kill_all_app_by_name "kafka.Kafka"
kill_all_app_by_name "QuorumPeerMain"
./bin/zookeeper-server-start.sh ./config/zookeeper.properties >> ./zk.log 2>&1 &
sleep 1
./bin/kafka-server-start.sh ./config/server.properties >> ./kafka.log 2>&1 &
~~~

创建主题、消费者以及生产者来进行测试。

~~~bash
./bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic order_service
~~~

~~~bash
./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic order_service
~~~

~~~bash
./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic order_service
~~~



### Filebeat

Filebeat 是一个轻量级的本地日志采集程序，其可以监听和采集日志目录或者特定的日志文件，并将日志文件的增量转发给 ES、Kafka、Logstash 等外部服务。

![filebeat.png](assets/892de87b6a044711bae7fa0f95232061tplv-k3u1fbpfcp-jj-mark1512000q75.webp)

Filebeat 主要由两部分组成

- Input ：找本地日志文件
- Harvester：监听和读取这日志文件的内容，并且发送日志内容到 libbeat
- libbeat 提供了聚合日志事件和发送聚合后数据到指定外部服务的功能



下载安装 Filebeat：

~~~bash
# 使用 wget 下载
wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.13.0-linux-x86_64.tar.gz

# 解压
tar xzvf filebeat-7.13.4-linux-x86_64.tar.gz

# 重命名
mv filebeat-7.13.4-linux-x86_64 filebeat
~~~

 Filebeat 的配置:

~~~bash

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /home/ubuntu/OrderService.log # 改变到你的日志路径

  #fields:
  #  level: debug
  #  review: 1

  multiline.type: pattern
  multiline.pattern: ^\[
  multiline.negate: true
  multiline.match: after

output.kafka:
  # initial brokers for reading cluster metadata
  hosts: ["localhost:9092"]

  # message topic selection + partitioning
  topic: 'order_service'
  partition.round_robin:
    reachable_only: false

  required_acks: 1
  compression: gzip
~~~

- **multiline.type**，默认值为 pattern，定义了使用何种方式聚合数据。pattern 是使用正则匹配，其他选项为 count，将会聚合指定行数的日志为一个事件。

- **multiline.pattern**，配置是一个正则表达式，用于匹配需要在同一事件中连续的行

- **multiline.negate**：Defines whether the pattern is negated. The default is `false`.如果设为 true，那么与 `multiline.pattern` 匹配的行将标识为新事件的开始，如果设为 false，则匹配的行将被认为是当前事件的延续。

- **multiline.match**：Specifies how Filebeat combines matching lines into an event. The settings are `after` or `before`.

  | Setting for `negate` | Setting for `match` | Example `pattern: ^b`                                        |
  | -------------------- | ------------------- | ------------------------------------------------------------ |
  | `false`              | `after`             | ![Lines a b b c b b become "abb" and "cbb"](https://www.elastic.co/guide/en/beats/filebeat/7.13/images/false-after-multi.png) |
  | `false`              | `before`            | ![Lines b b a b b c become "bba" and "bbc"](https://www.elastic.co/guide/en/beats/filebeat/7.13/images/false-before-multi.png) |
  | `true`               | `after`             | ![Lines b a c b d e become "bac" and "bde"](https://www.elastic.co/guide/en/beats/filebeat/7.13/images/true-after-multi.png) |
  | `true`               | `before`            | ![Lines a c b d e b become "acb" and "deb"](assets/true-before-multi.png) |

- multiline.timeout：默认值为 5 秒，即 5 秒后将无法成为日志事件的内容变为日志事件，并且发送到 output 中去





启动 Filebeat：

~~~bash
# 修改权限
chmod go-w filebeat.yml

# 前台启动
./filebeat -e

# 后台启动，日志在 logs 目录下
./filebeat -c filebeat.yml >> /dev/null 2>&1 &
~~~



### **Logstash**

**Logstash 简单来说就是一个数据处理管道（ pipeline）** 。既然是数据处理管道，其必定有数据输入、数据处理、数据输出这个三个功能，这些在 Logstash 中对应的概念为 input、filter、output。

![logstash架构.png](assets/e642443fe8f047b7b7e76b44e8318732tplv-k3u1fbpfcp-jj-mark1512000q75.webp)



Logstash 安装：

~~~bash
# 下载 tar 包
wget https://artifacts.elastic.co/downloads/logstash/logstash-7.13.0-linux-x86_64.tar.gz

# 解压
tar xvf logstash-7.13.0-linux-x86_64.tar.gz

# 重命名
mv logstash-7.13.0 logstash
~~~

进入 logstash 的 config 目录，创建 OrderServiceLog.conf 配置文件：

~~~bash
# 进入配置目录
cd logstash/config

# 创建 OrderServiceLog.conf 配置文件
touch OrderServiceLog.conf

# 将以下内容写入到到 OrderServiceLog.conf 中
input {
    kafka {
        bootstrap_servers => ["localhost:9092"]
        topics => ["order_service"]
        codec => "json"
        auto_offset_reset => "latest"
        decorate_events=> "basic"
        group_id => "logstash01"
    }
}

filter {
    grok {
        match => { "message" => "(?m)^\[(?<log_date>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3})\]%{SPACE}\[(?<level>[A-Z]{4,5})\]" }
    }

    mutate {
        lowercase => [ "level" ]
        remove_field => ["beat"]
    }
}

output {
   elasticsearch {
        hosts => ["https://localhost:9211"]
        index =>  "%{[@metadata][kafka][topic]}_%{+YYYY-MM-dd}"
        user => "elastic" 
        password => "123456"
        ssl => true
        ssl_certificate_verification => false
        cacert => "/home/ubuntu/ES/logstash/config/es_ca.pem"
    }
}
~~~

**kafka input 插件**“：

- **bootstrap_servers**：指定 Kafka 集群服务地址
- **topics**，指定要消费的 topics
- **codec**，输入数据的格式，默认是 "plain"



运行

~~~bash
nohup ./bin/logstash -f config/OrderServiceLog.conf >> ./run.log 2>&1 &
~~~



###  Kibana

