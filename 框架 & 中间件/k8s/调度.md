# 调度

[TOC]

`kube-scheduler` 根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源。

![kube-scheduler structrue](./assets/kube-scheduler-overview.png)

考虑到实际环境中的各种复杂情况，kubernetes 的调度器采用插件化的形式实现，可以方便用户进行定制或者二次开发

调度主要分为以下几个部分：

- 首先是**预选过程**，过滤掉不满足条件的节点，这个过程称为`Predicates`（过滤）
- 然后是**优选过程**，对通过的节点按照优先级排序，称之为`Priorities`（打分）
- 最后从中选择优先级最高的节点，如果中间任何一步骤有错误，就直接返回错误

如果所有的节点都不满足条件，那么 Pod 将会一直处于 Pending 状态，直到有节点满足条件，在这期间调度器会不断的重试。

默认情况下调度器考虑的是资源足够，并且负载尽量平均，但是有的时候我们需要能够更加细粒度的去控制 Pod 的调度，这里就引入以下概念：

- **节点亲和性（nodeAffinity）**：控制 Pod 要部署在哪些节点上，以及不能部署在哪些节点上的
- **Pod 亲和性（podAffinity）**：解决 Pod 可以和哪些 Pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等）
- **Pod 反亲和性（podAntiAffinity）**：如果一个节点上运行了某个 Pod，那么我们的 Pod **就不**调度到那个节点上。



（反）亲和性调度可以分成**软策略**和**硬策略**两种方式:

- **软策略（preferredDuringSchedulingIgnoredDuringExecution）**：如果现在没有满足调度要求的节点的话，Pod 就会忽略这条规则，继续完成调度过程。
- **硬策略（requiredDuringSchedulingIgnoredDuringExecution）**：如果没有满足条件的节点的话，就不断重试直到满足条件为止。

## NodeSelector

不过，我们先介绍控制粒度偏大的 NodeSelector。只需要在 Pod 的 spec 字段中添加 `nodeSelector` 字段，这样该 Pod 就会分配到指定标签的节点上了：

~~~yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: busybox-pod
  name: test-busybox
spec:
  containers:
  # ...
  nodeSelector:
    com: youdianzhishi		# 匹配 com=youdianzhishi 具有标签的节点
~~~

不过需要注意的是`nodeSelector` 属于强制性的，如果我们的目标节点没有可用的资源，我们的 Pod 就会一直处于 `Pending` 状态。

我们通过以下命令来查看 Node 的标签：

~~~shell
$ kubectl get nodes --show-labels
~~~

通过 `kubectl label` 来给节点添加标签：

~~~shell
$ kubectl label nodes ydzs-node2 com=youdianzhishi # 增加了一个com=youdianzhishi 标签
~~~



## nodeAffinity

下面给出 nodeAffinity 的例子：

~~~yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-affinity
  labels:
    app: node-affinity
spec:
  replicas: 8
  selector:
    matchLabels:
      app: node-affinity
  template:
      # 。。。
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
            nodeSelectorTerms:
            # 不能运行在携带 hostname=ydzs-node3 标签的节点上
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: NotIn
                values:
                - ydzs-node3
          preferredDuringSchedulingIgnoredDuringExecution:  # 软策略
          - weight: 1
            preference:
            # 优先调度到 com=youdianzhishi 节点
              matchExpressions:
              - key: com
                operator: In
                values:
                - youdianzhishi
~~~

现在 Kubernetes 提供的操作符有下面的几种：

- In：label 的值在某个列表中
- NotIn：label 的值不在某个列表中
- Gt：label 的值大于某个值
- Lt：label 的值小于某个值
- Exists：某个 label 存在
- DoesNotExist：某个 label 不存在

如果 `nodeSelectorTerms` 下面有多个 `matchExpressions` 的话，满足任何一个条件就可以了；如果 `matchExpressions`有多个选项的话，则必须同时满足这些条件

## podAffinity

Pod 亲和性的例子：

~~~yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-affinity
  labels:
    app: pod-affinity
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pod-affinity
  template:
     # ...
      affinity:
      	# 被分配到的节点，必须具有携带 app: busybox-pod 标签的节点
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - busybox-pod
            # 我们当前调度的 Pod 要和目标的 Pod 处于同一个主机上面
            topologyKey: kubernetes.io/hostname
~~~

## podAntiAffinity

podAntiffinity 的例子：

~~~yaml
template:
	# 与 podAffinity 类似，仅仅改了这个标签而已
	podAntiAffinity:
~~~

## 污点与容忍

把一组具有特殊资源预留给某些 Pod，则污点就很有用了。如果一个节点标记为 Taints ，除非将标识  Pod  为可以容忍污点的节点，否则该 Pod 不会被调度到该节点。

一个污点节点有以下参数：

- `NodeSchedule`：Pod 不会被调度到标记为 taints 的节点
- `PreferNoSchedule`：NoSchedule 的软策略版本，表示尽量不调度到污点节点上去
- `NoExecute`：一旦 Taint 生效，如该节点内正在运行的 Pod 没有对应容忍（Tolerate）设置，则会直接被逐出

设置污点节点的命令如下：

~~~shell
$ kubectl taint nodes ydzs-node2 test=node2:NoSchedule
~~~

取消污点：

~~~shell
$ kubectl taint nodes ydzs-node2 test-
~~~



查看节点的污点情况：

~~~shell
$ kubectl describe node ydzs-master
Name:               ydzs-master
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
......
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
......
~~~



通过 spec.tolerations 来设置 Pod 所容忍的污点节点：

~~~yaml
apiVersion: apps/v1
kind: Deployment
#...
spec:
  #...
  template:
    #...
    spec:
      #...
      tolerations:
      # key、value、effect 与 Node 的 Taint 设置需保持一致
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
~~~

